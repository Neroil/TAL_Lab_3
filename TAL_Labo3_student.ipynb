{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\" align=\"right\" /> \n",
    "\n",
    "# Cours TAL - Laboratoire 3<br/>*Depedency parser* pour le français dans spaCy\n",
    "\n",
    "**Objectif**\n",
    "\n",
    "Évaluer l'analyseur syntaxique en dépendances fourni par spaCy dans le modèle `fr_core_news_sm`, puis le comparer avec un analyseur entraîné par vous-mêmes.  Les données sont les mêmes qu'au Labo 2 et la démarche du labo est similaire aussi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prise en main de l'analyseur de spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\") # charge la pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** Pour la pipeline `fr_core_news_sm`, veuillez afficher les traitements disponibles, puis désactiver tous les traitements sauf `tok2vec`, `morphologizer` et `parser`, puis vérifiez que la désactivation a bien fonctionné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitements disponibles : ['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Traitements activés : ['tok2vec', 'morphologizer', 'parser']\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "print(\"Traitements disponibles :\", nlp.pipe_names)\n",
    "for pipe in nlp.pipe_names:\n",
    "    if pipe not in {\"tok2vec\", \"morphologizer\",\"parser\"}:\n",
    "        nlp.disable_pipe(pipe)\n",
    "\n",
    "print(\"Traitements activés :\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr.examples import sentences\n",
    "s1 = sentences[2] # prenons la 3e phrase comme exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** Veuillez analyser `s1` avec la pipeline `nlp` puis afficher chaque token, son POS tag, et son étiquette indiquant la relation de dépendance (entre crochets, après le token).  Quelle information essentielle manque dans cette représentation ?\n",
    "\n",
    "Note : le *morphologizer* fournit aussi les POS tags.  La liste des tags possibles est [fournie par spaCy](https://spacy.io/models/fr#fr_core_news_md-labels).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco envisage d'interdire les robots coursiers sur les trottoirs\n",
      "San [PRON, nsubj, head: envisage]\n",
      "Francisco [PROPN, flat:name, head: San]\n",
      "envisage [VERB, ROOT, head: envisage]\n",
      "d' [ADP, case, head: interdire]\n",
      "interdire [NOUN, obl:arg, head: envisage]\n",
      "les [DET, det, head: robots]\n",
      "robots [NOUN, obj, head: envisage]\n",
      "coursiers [ADJ, amod, head: robots]\n",
      "sur [ADP, case, head: trottoirs]\n",
      "les [DET, det, head: trottoirs]\n",
      "trottoirs [NOUN, nmod, head: robots]\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code et votre réponse ici.\n",
    "doc = nlp(s1)\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(f\"{token.text} [{token.pos_}, {token.dep_}, head: {token.head.text}]\")\n",
    "\n",
    "\n",
    "# Il manquait le head dans le code précédent. Pourquoi c'est important ? Car il permet de savoir quel est le mot qui gouverne le mot courant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1c.** Veuillez afficher tous les groupes de mots qui sont soit des `nsubj` soit des `obj` dans la phrase `s1` (c'est à dire les sujets et les objets du verbe).   Indication : le sous-arbre d'un token *t* est accessible comme `t.subtree`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsubj : San [PRON, nsubj, head: envisage]\n",
      "\tSan [PRON, nsubj, head: envisage]\n",
      "\tFrancisco [PROPN, flat:name, head: San]\n",
      "obj : robots [NOUN, obj, head: envisage]\n",
      "\tles [DET, det, head: robots]\n",
      "\trobots [NOUN, obj, head: envisage]\n",
      "\tcoursiers [ADJ, amod, head: robots]\n",
      "\tsur [ADP, case, head: trottoirs]\n",
      "\tles [DET, det, head: trottoirs]\n",
      "\ttrottoirs [NOUN, nmod, head: robots]\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "for token in doc:\n",
    "    if token.dep_ in [\"nsubj\", \"obj\"]:\n",
    "        print(f\"{token.dep_} : {token.text} [{token.pos_}, {token.dep_}, head: {token.head.text}]\")\n",
    "        for t in token.subtree:\n",
    "            print(f\"\\t{t.text} [{t.pos_}, {t.dep_}, head: {t.head.text}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Évaluation quantitative de l'analyseur sur une phrase \n",
    "\n",
    "Les données sont les mêmes que celles du Labo 2.  Vous les avez déjà transformées au Labo 2 dans un format utilisable par spaCy, dans un dossier nommé `Labo2/spacy_data` que vous allez réutiliser.  Les trois fichiers contiennent des phrases en français annotées aussi avec les arbres de dépendance.  Le fichier `fr-ud-train.conllu` est destiné à l'entraînement, `fr-ud-dev.conllu` au réglage des paramètres, et `fr-ud-test.conllu` à l'évaluation finale.\n",
    "\n",
    "**2a.** En inspectant un des fichiers d'origine avec un éditeur texte, veuillez indiquer dans quelles colonnes se trouvent les informations sur les relations de dépendance, et comment elles sont représentées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire votre réponse dans cette cellule.\n",
    "# La colonne 7 donne l’identifiant du mot auquel le mot courant est rattaché (le \"gouverneur\"), ou 0 si c’est le mot racine.\n",
    "# La colonne 8 indique le type de relation de dépendance (comme sujet, objet, etc.).\n",
    "# La colonne 9 peut contenir des dépendances supplémentaires sous forme de paires (gouverneur, relation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin, Doc\n",
    "test_data = DocBin().from_disk(\"spacy_data/fr-ud-test.spacy\")\n",
    "# for doc in test_data.get_docs(nlp.vocab):  # exemple\n",
    "#     for sent in doc.sents:\n",
    "#         print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b**. On rapplle que les données des fichiers convertis peuvent être chargées dans un objet de type `DocBin`.  Ici, un tel objet contient un ensemble de documents, chacun contenant 10 phrases.  Chaque document est un objet de type `Doc`.  Le code donné ci-dessous vous permet de charger les données de test et vous montre comment les afficher.\n",
    "\n",
    "* Veuillez stocker la *7e phrase du 2e document des données de test* dans une variable nommée `s2`.\n",
    "* Veuillez afficher cette phrase (elle commence par \"Trois ans\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase : Trois ans plus tard, il tient un discours sur la crise.\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "sents = list(test_data.get_docs(nlp.vocab))\n",
    "s2 = list(sents[1].sents)[6]\n",
    "\n",
    "print(f\"Phrase : {s2.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** En utilisant `displaCy` comme expliqué [ici](https://spacy.io/usage/visualizers) veuillez afficher graphiquement l'arbre de dépendances de la phrase `s2` tel qu'il est fourni dans les données.  Pour être affichée, la phrase doit être transformée en objet `Doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fr\" id=\"105b80baeb7e4abca64956b65fa3695e-0\" class=\"displacy\" width=\"1975\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Trois</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">ans</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">plus</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">tard,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">il</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">tient</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">un</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">discours</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">sur</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">la</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">crise.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-105b80baeb7e4abca64956b65fa3695e-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-105b80baeb7e4abca64956b65fa3695e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-105b80baeb7e4abca64956b65fa3695e-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-105b80baeb7e4abca64956b65fa3695e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-105b80baeb7e4abca64956b65fa3695e-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-105b80baeb7e4abca64956b65fa3695e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-105b80baeb7e4abca64956b65fa3695e-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-105b80baeb7e4abca64956b65fa3695e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-105b80baeb7e4abca64956b65fa3695e-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-105b80baeb7e4abca64956b65fa3695e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-105b80baeb7e4abca64956b65fa3695e-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-105b80baeb7e4abca64956b65fa3695e-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-105b80baeb7e4abca64956b65fa3695e-0-6\" stroke-width=\"2px\" d=\"M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-105b80baeb7e4abca64956b65fa3695e-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,266.5 L1278.0,254.5 1262.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-105b80baeb7e4abca64956b65fa3695e-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-105b80baeb7e4abca64956b65fa3695e-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-105b80baeb7e4abca64956b65fa3695e-0-8\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-105b80baeb7e4abca64956b65fa3695e-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-105b80baeb7e4abca64956b65fa3695e-0-9\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,2.0 1800.0,2.0 1800.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-105b80baeb7e4abca64956b65fa3695e-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1800.0,266.5 L1808.0,254.5 1792.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML('<span class=\"tex2jax_ignore\">{}</span>'.format(\n",
    "    displacy.render(s2, style=\"dep\", jupyter=False)\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d.** En utilisant `displaCy`, veuillez également afficher l'arbre de dépendances calculé par la pipeline `nlp` pour cette même phrase `s2`.  Pour être analysée et affichée, la phrase doit être transformée en objet `Doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fr\" id=\"16f68956db0141ddb3ad70a4fbcf2af2-0\" class=\"displacy\" width=\"1975\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Trois</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">ans</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">plus</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">tard,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">il</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">tient</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">un</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">discours</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">sur</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">la</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">crise.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl:mod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,89.5 920.0,89.5 920.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-6\" stroke-width=\"2px\" d=\"M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,266.5 L1278.0,254.5 1262.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-8\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-9\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,2.0 1800.0,2.0 1800.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16f68956db0141ddb3ad70a4fbcf2af2-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1800.0,266.5 L1808.0,254.5 1792.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "doc2 = nlp(s2.as_doc())\n",
    "display(HTML('<span class=\"tex2jax_ignore\">{}</span>'.format(\n",
    "    displacy.render(doc2, style=\"dep\", jupyter=False)\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2e.** Veuillez comparer les deux arbres de dépendances et indiquer ici les différences.  Quel est le taux de correction de la pipeline `nlp` sur cette phrase ?\n",
    "\n",
    "Suggestion : il peut être utile de sauvegarder les deux arbres dans des images SVG, en écrivant dans un fichier le résultat retourné par `displacy.render` avec l'option `jupyter = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de correction : 84.62%\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre réponse ici.\n",
    "with open(\"original_tree.svg\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(displacy.render(s2, style=\"dep\", jupyter=False))\n",
    "    \n",
    "with open(\"predicted_tree.svg\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(displacy.render(doc2, style=\"dep\", jupyter=False))\n",
    "\n",
    "# On peut voir que la différence se trouve dans le groupe \"ans plus tard il tient\"\n",
    "# L'arbre original analyse correctement \"il\" comme sujet de \"tient\", qui est la racine de la phrase.\n",
    "# Dans l'arbre prédit, \"il\" est incorrectement rattaché à \"tard\", et \"tient\" n'est plus la racine.\n",
    "# Cela fausse toute la structure sujet-verbe et la relation temporelle \"ans plus tard\".\n",
    "\n",
    "original_tokens = [(token.text, token.dep_, token.head.text) for token in s2]\n",
    "predicted_tokens = [(token.text, token.dep_, token.head.text) for token in doc2]\n",
    "\n",
    "# Calculer le taux de correction\n",
    "correct = sum(1 for g, p in zip(original_tokens, predicted_tokens) if g == p)\n",
    "total = len(original_tokens)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Taux de correction : {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2f.**  Veuillez appliquer le `Scorer` de spaCy (voir Labo 2) et afficher les deux scores qu'il produit pour l'analyse en dépendances (avec trois décimales après la virgule).  Retrouvez-vous les scores de la question précédente ? Pourquoi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS : 0.818\n",
      "LAS : 0.818\n",
      "\n",
      "Comparaison avec le calcul manuel précédent:\n",
      "Taux de correction calculé manuellement: 0.846\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code et votre réponse ici.\n",
    "scorer = Scorer()\n",
    "\n",
    "scores = scorer.score([Example(nlp(s2.as_doc()), s2.as_doc())])\n",
    "\n",
    "print(f\"UAS : {scores['dep_uas']:.3f}\")\n",
    "print(f\"LAS : {scores['dep_las']:.3f}\")\n",
    "\n",
    "# Comparer avec le calcul manuel précédent\n",
    "print(f\"\\nComparaison avec le calcul manuel précédent:\")\n",
    "# 0.846\n",
    "print(f\"Taux de correction calculé manuellement: {accuracy:.3f}\")\n",
    "\n",
    "# On trouve en effet un score différent.\n",
    "# Le score obtenu avec `scorer.score()` est plus rigoureux car il s'appuie sur l'identifiant du mot gouverneur (head) (et pas juste son texte)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Évaluation du *dependency parser* de `fr_core_news_sm` sur l'ensemble des phrases test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a.** Veuillez calculer les deux scores qui caractérisent l'analyseur en dépendances de la pipeline `nlp` sur toutes les données de test présentes dans `test_data`.  Comment se comparent ces scores avec ceux mentionnés [dans la documentation de fr_core_news_sm](https://spacy.io/models/fr#fr_core_news_sm) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS : 0.760\n",
      "LAS : 0.619\n",
      "\n",
      "Comparaison avec les scores de la documentation:\n",
      "Documentation fr_core_news_sm: UAS = 88%, LAS = 84%\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "examples = []\n",
    "# Parcourir tous les documents et phrases dans les données de test\n",
    "for doc in test_data.get_docs(nlp.vocab):\n",
    "    for sent in doc.sents:\n",
    "        \n",
    "        parsed_sent = nlp(sent.text)\n",
    "        example = Example(sent.as_doc(), parsed_sent)\n",
    "        examples.append(example)\n",
    "\n",
    "scores = scorer.score(examples)\n",
    "\n",
    "print(f\"UAS : {scores['dep_uas']:.3f}\")\n",
    "print(f\"LAS : {scores['dep_las']:.3f}\")\n",
    "\n",
    "print(\"\\nComparaison avec les scores de la documentation:\")\n",
    "print(\"Documentation fr_core_news_sm: UAS = 88%, LAS = 84%\")\n",
    "\n",
    "#On voit qu'ils sont bien moins bon qu'advertised ... TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Le *scorer* fournit également des scores détaillés pour chaque type de relation de dépendances.  Veuillez afficher ces valeurs dans un tableau proprement formaté, trié par score F1 décroissant, avec trois décimales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Relation  Precision  Recall  F1-Score\n",
      "         det      0.770   0.928     0.842\n",
      "        case      0.775   0.854     0.813\n",
      "          cc      0.794   0.797     0.795\n",
      "  nsubj:pass      0.860   0.683     0.761\n",
      "        mark      0.779   0.723     0.750\n",
      "    aux:pass      0.925   0.628     0.748\n",
      "         cop      0.741   0.720     0.730\n",
      "       nsubj      0.676   0.757     0.714\n",
      "      nummod      0.664   0.750     0.705\n",
      "        root      0.795   0.617     0.695\n",
      "      advmod      0.700   0.678     0.689\n",
      "         obj      0.694   0.684     0.689\n",
      "        amod      0.743   0.588     0.656\n",
      "   flat:name      0.632   0.610     0.621\n",
      "        nmod      0.594   0.539     0.565\n",
      "       xcomp      0.613   0.489     0.544\n",
      "   acl:relcl      0.547   0.500     0.522\n",
      "       fixed      0.337   0.523     0.410\n",
      "         acl      0.416   0.403     0.409\n",
      "        iobj      0.560   0.318     0.406\n",
      "       advcl      0.382   0.402     0.392\n",
      "       ccomp      0.317   0.465     0.377\n",
      "        conj      0.370   0.381     0.375\n",
      "       appos      0.265   0.437     0.330\n",
      "   parataxis      0.031   0.048     0.038\n",
      "   aux:tense      0.000   0.000     0.000\n",
      "         dep      0.000   0.000     0.000\n",
      "     obl:arg      0.000   0.000     0.000\n",
      "         obl      0.000   0.000     0.000\n",
      "   obl:agent      0.000   0.000     0.000\n",
      "     obl:mod      0.000   0.000     0.000\n",
      "         aux      0.000   0.000     0.000\n",
      "   expl:subj      0.000   0.000     0.000\n",
      "   expl:comp      0.000   0.000     0.000\n",
      "        expl      0.000   0.000     0.000\n",
      "   obj:agent      0.000   0.000     0.000\n",
      "    aux:caus      0.000   0.000     0.000\n",
      "  nsubj:caus      0.000   0.000     0.000\n",
      "    compound      0.000   0.000     0.000\n",
      "   expl:pass      0.000   0.000     0.000\n",
      "  iobj:agent      0.000   0.000     0.000\n",
      "   discourse      0.000   0.000     0.000\n",
      "flat:foreign      0.000   0.000     0.000\n",
      "    vocative      0.000   0.000     0.000\n",
      "       csubj      0.000   0.000     0.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scores_per_type = scores[\"dep_las_per_type\"]\n",
    "df = pd.DataFrame.from_dict(scores_per_type, orient=\"index\")\n",
    "df.reset_index(inplace=True)\n",
    "df.columns = [\"Relation\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "df = df.sort_values(by=\"F1-Score\", ascending=False)\n",
    "df = df.round(3)\n",
    "print(df.to_string(index=False))\n",
    "df_before = df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraîner puis évaluer un nouveau *parser* français dans spaCy\n",
    "\n",
    "Le but de cette partie est d'entraîner une pipeline spaCy pour le français sur les données de `fr-ud-train.conllu`, puis de comparer le modèle obtenu avec le modèle prêt-à-l'emploi testé au point précédent (voir le Labo 2 et les [instructions de spaCy](https://spacy.io/usage/training#quickstart)).\n",
    "\n",
    "**4a.** Paramétrage de l'entraînement :\n",
    "* générez un fichier de départ grâce à [l'interface web](https://spacy.io/usage/training#quickstart), en indiquant que vous gardez seulement les composants `morphologizer` et `parser` dans la pipeline ;\n",
    "* sauvegardez le code généré par spaCy dans un fichier local `base_config.cfg` ;\n",
    "* générez un fichier `config.cfg` sur votre ordinateur en exécutant la ligne de commande suivante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez effectuer l'entraînement avec la ligne de commande suivante.  Faites plusieurs essais, d'abord avec un petit nombre d'époques (*à indiquer dans config.cfg*), pour estimer le temps nécessaire et observer les messages affichés.  Augmentez progressivement le nombre d'époques, jusqu'à ce que les scores sur le jeu de validation n'augmentent plus (si vous avez le temps).  Pendant combien d'époques entraînez-vous au final ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 21:01:56,703] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;4mℹ Saving to output directory: myDEPparser1\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2025-04-02 21:01:56,902] [INFO] Set up nlp object from config\n",
      "[2025-04-02 21:01:56,912] [DEBUG] Loading corpus from path: spacy_data/fr-ud-dev.spacy\n",
      "[2025-04-02 21:01:56,913] [DEBUG] Loading corpus from path: spacy_data/fr-ud-train.spacy\n",
      "[2025-04-02 21:01:56,913] [INFO] Pipeline: ['tok2vec', 'parser', 'morphologizer']\n",
      "[2025-04-02 21:01:56,916] [INFO] Created vocabulary\n",
      "[2025-04-02 21:01:56,916] [INFO] Finished initializing nlp object\n",
      "[2025-04-02 21:02:03,835] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, grc, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2025-04-02 21:02:12,175] [DEBUG] [W033] Training a new morphologizer using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, grc, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2025-04-02 21:02:19,249] [INFO] Initialized pipeline components: ['tok2vec', 'parser', 'morphologizer']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2025-04-02 21:02:19,261] [DEBUG] Loading corpus from path: spacy_data/fr-ud-dev.spacy\n",
      "[2025-04-02 21:02:19,262] [DEBUG] Loading corpus from path: spacy_data/fr-ud-train.spacy\n",
      "[2025-04-02 21:02:19,265] [DEBUG] Removed existing output directory: myDEPparser1/model-best\n",
      "[2025-04-02 21:02:19,266] [DEBUG] Removed existing output directory: myDEPparser1/model-last\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'parser', 'morphologizer']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS PARSER  LOSS MORPH...  DEP_UAS  DEP_LAS  SENTS_F  POS_ACC  MORPH_ACC  SCORE \n",
      "---  ------  ------------  -----------  -------------  -------  -------  -------  -------  ---------  ------\n",
      "  0       0          0.00       484.02         210.16    15.52     5.06     0.00    32.56      30.35    0.21\n",
      "  0     200       3937.07     36258.14       18014.72    60.62    53.41    70.63    82.67      78.43    0.69\n",
      "  0     400       6246.76     26155.69        9305.99    66.20    60.33    88.34    86.00      83.47    0.74\n",
      "  0     600       6314.50     22737.18        7053.37    67.97    62.72    92.08    87.00      85.19    0.76\n",
      "  0     800       6727.08     22017.41        6333.90    69.27    64.56    92.75    87.50      86.09    0.77\n",
      "  0    1000       6626.24     20268.06        5458.66    69.67    65.24    95.59    87.92      86.70    0.77\n",
      "  0    1200       6950.01     19940.80        5288.58    70.57    65.94    95.05    88.05      87.07    0.78\n",
      "  0    1400       7138.24     19278.22        4857.10    70.87    66.35    95.28    88.25      87.39    0.78\n",
      "  1    1600       7173.56     17711.07        4185.40    71.28    67.08    95.21    88.43      87.74    0.79\n",
      "  1    1800       7538.39     16792.89        3842.09    71.41    67.36    94.81    88.50      87.82    0.79\n",
      "  1    2000       7954.44     17230.94        3940.68    71.71    67.70    95.67    88.59      88.04    0.79\n",
      "  1    2200       8105.61     17127.05        3820.47    71.83    68.18    95.60    88.74      88.18    0.79\n",
      "  1    2400       8186.55     16976.10        3900.94    72.18    68.59    95.71    88.86      88.29    0.79\n",
      "  1    2600       8808.24     17775.58        3970.84    71.98    68.28    96.29    88.78      88.22    0.79\n",
      "  1    2800       8935.00     17319.63        3836.70    72.48    68.72    95.80    88.94      88.52    0.80\n",
      "  2    3000       8886.14     16701.11        3604.82    72.73    69.11    96.11    88.99      88.56    0.80\n",
      "  2    3200       8949.13     15226.28        3040.16    72.65    69.01    95.07    89.06      88.66    0.80\n",
      "  2    3400       9464.69     15553.45        3229.81    72.94    69.42    95.87    89.01      88.64    0.80\n",
      "  2    3600       9894.34     15759.53        3277.60    72.77    69.29    96.53    89.13      88.75    0.80\n",
      "  2    3800       9774.69     15206.19        3166.37    72.97    69.42    96.63    89.14      88.66    0.80\n",
      "  2    4000      10129.23     15150.87        2926.34    72.59    69.06    94.76    89.16      88.76    0.80\n",
      "  2    4200      10321.07     15089.39        3072.53    73.07    69.46    96.13    89.19      88.84    0.80\n",
      "  3    4400       9985.45     14661.46        3009.57    73.31    69.94    96.22    89.27      88.84    0.80\n",
      "  3    4600      10808.04     14462.20        2761.75    73.00    69.61    96.19    89.19      88.79    0.80\n",
      "  3    4800      10950.58     13983.13        2739.00    73.07    69.63    96.29    89.16      88.91    0.80\n",
      "  3    5000      11493.40     14203.92        2775.62    73.19    69.88    95.89    89.23      88.99    0.80\n",
      "  3    5200      12055.83     14626.28        2671.30    73.41    69.67    95.65    89.21      88.90    0.80\n",
      "  3    5400      11978.14     14217.96        2714.32    73.33    70.06    96.36    89.21      88.82    0.80\n",
      "  3    5600      12157.46     13850.25        2771.03    73.37    69.96    95.05    89.20      88.84    0.80\n",
      "  3    5800      12452.50     14345.85        2796.61    73.70    70.21    96.12    89.35      88.99    0.81\n",
      "  4    6000      12020.30     13179.54        2283.29    73.72    70.51    96.30    89.32      88.94    0.81\n",
      "  4    6200      13410.58     13205.88        2519.85    73.48    70.16    96.46    89.24      88.93    0.80\n",
      "  4    6400      13357.03     12973.81        2431.75    73.92    70.57    96.19    89.17      88.87    0.81\n",
      "  4    6600      14691.05     13799.43        2507.15    73.89    70.62    96.02    89.30      88.96    0.81\n",
      "  4    6800      14301.81     13503.13        2417.87    73.71    70.34    96.33    89.29      89.01    0.81\n",
      "  4    7000      14527.40     13606.96        2598.04    73.54    70.27    96.09    89.32      89.00    0.81\n",
      "  4    7200      14281.28     13221.50        2458.44    73.76    70.55    96.58    89.33      88.99    0.81\n",
      "  5    7400      14788.45     13057.73        2444.72    73.93    70.84    96.51    89.33      89.00    0.81\n",
      "  5    7600      14832.61     12265.98        2147.04    73.86    70.57    96.57    89.29      89.03    0.81\n",
      "  5    7800      16209.07     12702.25        2236.80    74.06    71.04    96.02    89.50      89.13    0.81\n",
      "  5    8000      16420.56     12819.07        2293.38    73.67    70.66    95.71    89.30      89.02    0.81\n",
      "  5    8200      17651.67     13388.26        2369.53    73.93    70.85    96.44    89.29      88.99    0.81\n",
      "  5    8400      16828.05     12703.79        2373.96    73.88    70.74    95.99    89.38      89.09    0.81\n",
      "  5    8600      16516.96     12294.35        2234.25    73.90    70.73    96.50    89.37      89.05    0.81\n",
      "  6    8800      17108.95     12762.47        2143.20    74.03    70.83    96.33    89.38      89.17    0.81\n",
      "  6    9000      17040.28     11464.14        1987.84    74.21    70.92    96.50    89.43      89.12    0.81\n",
      "  6    9200      19382.45     12036.84        2140.38    74.23    70.87    96.06    89.45      89.08    0.81\n",
      "  6    9400      19050.06     11865.74        2064.79    73.78    70.50    96.44    89.43      89.08    0.81\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "myDEPparser1/model-last\n"
     ]
    }
   ],
   "source": [
    "# Note : il vaut mieux exécuter cela directement dans une fenêtre de commande, pour voir les logs en temps réel.\n",
    "!python -m spacy train config.cfg \\\n",
    "  --output ./myDEPparser1 \\\n",
    "  --paths.train ./spacy_data/fr-ud-train.spacy \\\n",
    "  --paths.dev ./spacy_data/fr-ud-dev.spacy \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez indiquer ici le nombre d'époques final. \n",
    "# L'entraînement du modèle s’est déroulé sur 7 époques complètes. On observe que les scores sur le jeu de validation (notamment DEP_UAS et DEP_LAS) ont progressivement augmenté jusqu’à environ la 5ᵉ ou 6ᵉ époque, puis se sont stabilisés.\n",
    "# Cela indique que le modèle a atteint un plateau de performance, et que prolonger l’entraînement au-delà de 7 époques n’aurait probablement pas apporté d’amélioration significative.\n",
    "\n",
    "# En analysant la progression du F1-score global (SCORE), on constate qu’il augmente régulièrement durant les premières époques, passant de 0.21 à 0.81.\n",
    "# En se basant uniquement sur cette métrique, on peut conclure que l’amélioration significative s’arrête vers l’époque 5. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.**  Veuillez charger le meilleur modèle (pipeline) dans la variable `nlp2` et afficher ses scores sur les données de test.  Comment se comparent les résultats avec les précédents ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS: 0.788\n",
      "LAS: 0.729\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "\n",
    "nlp2 = spacy.load(\"./myDEPparser1/model-best\")\n",
    "data_test = DocBin().from_disk(\"spacy_data/fr-ud-test.spacy\")\n",
    "\n",
    "scorer = Scorer().score([\n",
    "    Example(\n",
    "        nlp2(Doc(nlp2.vocab, [token.text for token in doc])),\n",
    "        doc\n",
    "    ) for doc in test_data.get_docs(nlp2.vocab)\n",
    "])\n",
    "\n",
    "print(f\"UAS: {scorer['dep_uas']:.3f}\")\n",
    "print(f\"LAS: {scorer['dep_las']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c.** Veuillez afficher les scores détaillés pour chaque type de relation de dépendances, dans un tableau formaté comme au 3b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Relation  Precision  Recall  F1-Score\n",
      "  aux:caus      1.000   0.923     0.960\n",
      "       det      0.961   0.863     0.909\n",
      "    nummod      0.854   0.842     0.848\n",
      "       aux      0.849   0.825     0.837\n",
      "      case      0.831   0.814     0.823\n",
      "      root      0.807   0.796     0.801\n",
      "       cop      0.778   0.806     0.792\n",
      "        cc      0.786   0.767     0.776\n",
      "      mark      0.813   0.714     0.760\n",
      "  aux:pass      0.676   0.868     0.760\n",
      "     nsubj      0.749   0.755     0.752\n",
      "      amod      0.657   0.877     0.752\n",
      "nsubj:caus      1.000   0.600     0.750\n",
      "       obj      0.698   0.763     0.729\n",
      "     xcomp      0.661   0.755     0.705\n",
      "nsubj:pass      0.673   0.740     0.705\n",
      "    advmod      0.712   0.697     0.704\n",
      " flat:name      0.620   0.739     0.674\n",
      "      nmod      0.654   0.603     0.627\n",
      "       obl      0.554   0.626     0.588\n",
      "     ccomp      0.551   0.603     0.576\n",
      "     advcl      0.573   0.578     0.576\n",
      " acl:relcl      0.591   0.520     0.553\n",
      "      conj      0.533   0.556     0.544\n",
      "       acl      0.537   0.524     0.530\n",
      "      iobj      0.545   0.480     0.511\n",
      "     fixed      0.817   0.331     0.472\n",
      "     appos      0.246   0.466     0.322\n",
      "      expl      0.667   0.179     0.282\n",
      " parataxis      0.278   0.156     0.200\n",
      "  compound      0.152   0.114     0.130\n",
      "       dep      0.000   0.000     0.000\n",
      " obj:agent      0.000   0.000     0.000\n",
      "iobj:agent      0.000   0.000     0.000\n",
      " obl:agent      0.000   0.000     0.000\n",
      " discourse      0.000   0.000     0.000\n",
      "     csubj      0.000   0.000     0.000\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "scores_per_type = scorer[\"dep_las_per_type\"]\n",
    "df = pd.DataFrame.from_dict(scores_per_type, orient=\"index\")\n",
    "df.reset_index(inplace=True)\n",
    "df.columns = [\"Relation\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "df = df.sort_values(by=\"F1-Score\", ascending=False)\n",
    "df = df.round(3)\n",
    "print(df.to_string(index=False))\n",
    "df_after = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de F1-score à 0 avant entraînement : 20\n",
      "Nombre de F1-score à 0 après entraînement : 6\n"
     ]
    }
   ],
   "source": [
    "# Compter le nombre de F1-scores à 0 pour chaque DataFrame\n",
    "def count_zero_f1(df):\n",
    "    return (df[\"F1-Score\"] == 0.0).sum()\n",
    "\n",
    "f1_zeros_before = count_zero_f1(df_before)  # df_before = tableau AVANT entraînement\n",
    "f1_zeros_after = count_zero_f1(df_after)    # df_after = tableau APRÈS entraînement\n",
    "\n",
    "print(f\"Nombre de F1-score à 0 avant entraînement : {f1_zeros_before}\")\n",
    "print(f\"Nombre de F1-score à 0 après entraînement : {f1_zeros_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4d.** Quels changements observez-vous en haut (3 premiers labels) et en bas du classement ?  Voyez-vous un label pour lequel les scores n'augmentent pas avec le parser entraîné ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dans les résultats obtenus, on remarque qu’un certain nombre de relations de dépendances conservent un score F1 nul, même après l'entraînement du parser. \n",
    "# Afin de quantifier cette observation, on compte le nombre de labels avec un F1-score égal à 0 avant et après entraînement. \n",
    "# Cette comparaison nous permet de voir qu'avant il y avait 20 labels avec un F1-score nul, et après il n'y en a plus que 6.\n",
    "\n",
    "# Concernant les 3 relations les moins bien reconnues, avant entraînement, il s’agissait de discourse, flat:foreign, et vocative (toutes avec un F1 = 0.000), ce qui indique une absence totale de reconnaissance.\n",
    "# Avant l'entraînement du parser, les 3 relations de dépendance les mieux reconnues étaient det (F1 = 0.842), case (F1 = 0.813) et cc (F1 = 0.795). Ces labels sont fréquents et relativement simples à identifier, ce qui explique leurs bons scores initiaux. \n",
    "# Après entraînement, on observe une amélioration générale avec en tête aux:caus (F1 = 0.960), det (F1 = 0.909) et nummod (F1 = 0.848). Cela montre que le modèle a su mieux reconnaître certaines relations, notamment nummod qui passe de la 9e à la 3e place (+0.143).\n",
    "# Après entraînement, ces mêmes labels restent en bas du classement, avec toujours un F1-score nul, montrant que le modèle n’a pas appris à les traiter — probablement en raison d’un manque de représentativité dans les données d’apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fin du Labo.** Veuillez nettoyer ce notebook en gardant seulement les résultats désirés, l'enregistrer, et le soumettre comme devoir sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CoursTAL",
   "language": "python",
   "name": "courstal_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
